{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'S3BucketName'\n",
    "prefix = 'S3Prefix'\n",
    "\n",
    "# import needed Python libraries\n",
    "import boto3\n",
    "import re\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import date, datetime, timedelta\n",
    "import io\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "from IPython.display import display\n",
    "from time import strftime, gmtime\n",
    "import sagemaker\n",
    "from sagemaker.predictor import csv_serializer\n",
    "\n",
    "# Define IAM role\n",
    "role = get_execution_role()\n",
    "\n",
    "# Read in the sample customer data\n",
    "churn = pd.read_csv('./ChurnSampleData.csv')\n",
    "\n",
    "# Begin feature engineering\n",
    "churn = churn.drop(['EndpointID','EmailAddress'], axis=1)\n",
    "\n",
    "# Transform unix timestamp event columns into buckets\n",
    "#   0 = more than 180 days ago\n",
    "#   1 = more than 30 days ago\n",
    "#   2 = less than or equal to 30 days ago\n",
    "bucket_start = date(2019,10,25)\n",
    "for index, row in churn.iterrows():\n",
    "    ts = date.fromtimestamp(row['LastEmailDelivered'])\n",
    "    if ts < bucket_start - timedelta(days=180):\n",
    "        val = 0\n",
    "    elif ts < bucket_start - timedelta(days=30):\n",
    "        val = 1\n",
    "    else:\n",
    "        val = 2\n",
    "    row['LastEmailDelivered'] = val\n",
    "    \n",
    "    ts = date.fromtimestamp(row['LastEmailEngaged'])\n",
    "    if ts < bucket_start - timedelta(days=180):\n",
    "        val = 0\n",
    "    elif ts < bucket_start - timedelta(days=30):\n",
    "        val = 1\n",
    "    else:\n",
    "        val = 2\n",
    "    row['LastEmailEngaged'] = val\n",
    "\n",
    "# format our model data with predictor variable in first column\n",
    "model_data = pd.concat([churn['Churn?'], churn.drop(['Churn?'], axis=1)], axis=1)\n",
    "\n",
    "# split data into training, validation, and test sets\n",
    "train_data, validation_data, test_data = np.split(model_data.sample(frac=1, random_state=1729), [int(0.7 * len(model_data)), int(0.9 * len(model_data))])\n",
    "train_data.to_csv('train.csv', header=False, index=False)\n",
    "validation_data.to_csv('validation.csv', header=False, index=False)\n",
    "model_data.to_csv('model_data.csv', header=True, index=False)\n",
    "\n",
    "# upload to S3\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train/train.csv')).upload_file('train.csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'validation/validation.csv')).upload_file('validation.csv')\n",
    "\n",
    "# begin model training\n",
    "container = get_image_uri(boto3.Session().region_name, 'xgboost', '0.90-1')\n",
    "s3_input_train = sagemaker.s3_input(s3_data='s3://{}/{}/train'.format(bucket, prefix), content_type='csv')\n",
    "s3_input_validation = sagemaker.s3_input(s3_data='s3://{}/{}/validation/'.format(bucket, prefix), content_type='csv')\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "xgb = sagemaker.estimator.Estimator(container,\n",
    "                                    role,\n",
    "                                    train_instance_count=1,\n",
    "                                    train_instance_type='ml.m4.xlarge',\n",
    "                                    output_path='s3://{}/{}/output2'.format(bucket, prefix),\n",
    "                                    sagemaker_session=sess)\n",
    "xgb.set_hyperparameters(max_depth=5,\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        silent=0,\n",
    "                        objective='binary:logistic',\n",
    "                        num_round=100)\n",
    "\n",
    "xgb.fit({'train': s3_input_train, 'validation': s3_input_validation})\n",
    "\n",
    "# compile model\n",
    "client = boto3.client('sagemaker')\n",
    "\n",
    "info = client.describe_training_job(TrainingJobName=xgb.latest_training_job.name)\n",
    "model_data = info['ModelArtifacts']['S3ModelArtifacts']\n",
    "\n",
    "primary_container = {\n",
    "    'Image': container,\n",
    "    'ModelDataUrl': model_data\n",
    "}\n",
    "\n",
    "# host model\n",
    "client.create_model(\n",
    "    ModelName = 'deployed-xgboost-customer-churn',\n",
    "    ExecutionRoleArn = role,\n",
    "    PrimaryContainer = primary_container)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
